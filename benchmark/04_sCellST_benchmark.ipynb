{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sCellST Benchmark (INT25-INT28)\n",
    "\n",
    "## Environment\n",
    "It is recommended to use a separate environment for sCellST.\n",
    "\n",
    "```bash\n",
    "conda create -n scellst_bench python=3.10\n",
    "conda activate scellst_bench\n",
    "\n",
    "# Install PyTorch (CUDA 11.8)\n",
    "pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Install other dependencies\n",
    "pip install scanpy pandas numpy matplotlib seaborn scprep tqdm pytorch-lightning omegaconf loguru anndata scikit-learn dask[dataframe]\n",
    "\n",
    "# Install sCellST dependencies if not covered above. Usually sCellST requires specific versions.\n",
    "# Assuming sCellST source code is in benchmark/sCellST\n",
    "\n",
    "# === IMPORTANT: OpenCV Fix ===\n",
    "# If you encounter \"DLL load failed\" for cv2, try installing headless version:\n",
    "pip uninstall opencv-python -y\n",
    "pip install opencv-python-headless\n",
    "```"
   ],
   "id": "9700b2dbd7011beb"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T11:20:48.366229300Z",
     "start_time": "2026-01-20T11:20:38.247380700Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# === Windows DLL Fix ===\n",
    "# Fix for OSError: [WinError 1114] Error loading c10.dll / libiomp5md.dll etc.\n",
    "if os.name == 'nt':\n",
    "    conda_prefix = os.environ.get('CONDA_PREFIX')\n",
    "    possible_paths = []\n",
    "    if conda_prefix:\n",
    "        possible_paths.append(Path(conda_prefix) / 'Library' / 'bin')\n",
    "    \n",
    "    # Add current python dir's Library/bin if distinct\n",
    "    curr_python = sys.executable\n",
    "    if curr_python:\n",
    "        possible_paths.append(Path(curr_python).parent / 'Library' / 'bin')\n",
    "\n",
    "    for p in possible_paths:\n",
    "        if p.exists():\n",
    "            print(f\"Adding DLL path: {p}\")\n",
    "            os.environ['PATH'] = str(p) + os.pathsep + os.environ['PATH']\n",
    "            try:\n",
    "                os.add_dll_directory(str(p))\n",
    "            except AttributeError:\n",
    "                pass \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# Set Paths\n",
    "ROOT = Path(os.environ.get('MORPHO_VC_ROOT', '../')).expanduser().resolve()\n",
    "BENCHMARK_DIR = ROOT / 'benchmark'\n",
    "SCELLST_DIR = BENCHMARK_DIR / 'sCellST'\n",
    "RESULT_DIR = BENCHMARK_DIR / 'results' / 'sCellST'\n",
    "DATA_DIR = ROOT / 'data'\n",
    "HEST_DIR = DATA_DIR / 'hest_data'\n",
    "\n",
    "# Add paths to sys.path\n",
    "# 1. sCellST\n",
    "sys.path.append(str(SCELLST_DIR))\n",
    "\n",
    "# 2. HEST (Local third_party)\n",
    "HEST_SRC = ROOT / 'third_party' / 'HEST' / 'src'\n",
    "if HEST_SRC.exists():\n",
    "    print(f\"Adding HEST local source: {HEST_SRC}\")\n",
    "    sys.path.insert(0, str(HEST_SRC))\n",
    "else:\n",
    "    print(\"Warning: local HEST source not found in third_party. If 'hest' is not installed via pip, this will fail.\")\n",
    "\n",
    "RESULT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"ROOT: {ROOT}\")\n",
    "print(f\"HEST_DIR: {HEST_DIR}\")\n",
    "print(f\"SCELLST_DIR: {SCELLST_DIR}\")\n",
    "\n",
    "# sCellST imports\n",
    "# Fix for dask-expr RuntimeError in spatialdata\n",
    "try:\n",
    "    import dask\n",
    "    dask.config.set({'dataframe.query-planning': False})\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    from scellst.submit_function import embed_cells\n",
    "    from scellst.dataset.data_module import STDataModule, MilVisiumHandler\n",
    "    from scellst.utils.io_utils import load_config\n",
    "    from scellst.utils.utils import prepare_model\n",
    "    from scellst.trainer import prepare_trainer\n",
    "    from omegaconf import OmegaConf, DictConfig\n",
    "    from lightning.pytorch import seed_everything\n",
    "    import lightning as L\n",
    "    import scellst.type\n",
    "    import scellst.lightning_model.gene_lightning_model\n",
    "except ImportError as e:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"IMPORT ERROR DETECTED\")\n",
    "    print(f\"Error: {e}\")\n",
    "    if 'cv2' in str(e):\n",
    "        print(\"\\nSUGGESTION: This looks like an OpenCV DLL error.\")\n",
    "        print(\"Try running the following in your terminal:\")\n",
    "        print(\"  pip uninstall opencv-python -y\")\n",
    "        print(\"  pip install opencv-python-headless\")\n",
    "    elif 'loguru' in str(e):\n",
    "        print(\"\\nSUGGESTION: Missing 'loguru'. Install it via:\")\n",
    "        print(\"  pip install loguru\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "# --- Fix 1: Runtime Monkeypatch for TaskType ---\n",
    "# Fixes 'AttributeError: att_regression' by adding missing enum members at runtime\n",
    "class PatchedTaskType(str, Enum):\n",
    "    regression = \"regression\"\n",
    "    nb_total_regression = \"nb_total_regression\"\n",
    "    nb_mean_regression = \"nb_mean_regression\"\n",
    "    # The missing members that cause the crash:\n",
    "    bag_regression = \"bag_regression\"\n",
    "    att_regression = \"att_regression\"\n",
    "\n",
    "    @classmethod\n",
    "    def list(cls):\n",
    "        return list(map(lambda c: c.value, cls))\n",
    "\n",
    "# Apply the patch\n",
    "scellst.type.TaskType = PatchedTaskType\n",
    "\n",
    "scellst.lightning_model.gene_lightning_model.TaskType = PatchedTaskType\n",
    "print(\"Success: Monkeypatched TaskType to fix library inconsistencies.\")"
   ],
   "id": "8fc08e16d362ced0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding DLL path: C:\\ProgramData\\anaconda3\\envs\\scellst_bench\\Library\\bin\n",
      "Adding DLL path: C:\\ProgramData\\anaconda3\\envs\\scellst_bench\\Library\\bin\n",
      "Adding HEST local source: D:\\code\\Morpho-VC\\third_party\\HEST\\src\n",
      "ROOT: D:\\code\\Morpho-VC\n",
      "HEST_DIR: D:\\code\\Morpho-VC\\data\\hest_data\n",
      "SCELLST_DIR: D:\\code\\Morpho-VC\\benchmark\\sCellST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\scellst_bench\\lib\\site-packages\\dask\\dataframe\\__init__.py:31: FutureWarning: The legacy Dask DataFrame implementation is deprecated and will be removed in a future version. Set the configuration option `dataframe.query-planning` to `True` or None to enable the new Dask Dataframe implementation and silence this warning.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\envs\\scellst_bench\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m19:20:41\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mPROJ_ROOT path is: D:\\code\\Morpho-VC\\benchmark\\sCellST\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\scellst_bench\\lib\\site-packages\\xarray_schema\\__init__.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Monkeypatched TaskType to fix library inconsistencies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\scellst_bench\\lib\\site-packages\\anndata\\utils.py:434: FutureWarning: Importing read_text from `anndata` is deprecated. Import anndata.io.read_text instead.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Data & Embeddings\n",
    "\n",
    "Load common genes and generate cell embeddings using sCellST's Pre-trained ResNet50 (ImageNet)."
   ],
   "id": "853a9a4c6d15129a"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T11:20:48.396414900Z",
     "start_time": "2026-01-20T11:20:48.367730900Z"
    }
   },
   "source": [
    "# Load Common Genes\n",
    "common_gene_path = DATA_DIR / 'spatial_data' / 'common_genes.txt'\n",
    "with open(common_gene_path, 'r') as f:\n",
    "    common_genes = [line.strip() for line in f.readlines()]\n",
    "print(f\"Loaded {len(common_genes)} common genes.\")\n",
    "\n",
    "# Slide Splits\n",
    "train_ids = ['INT25', 'INT26']\n",
    "val_ids = ['INT27']\n",
    "test_ids = ['INT28']\n",
    "all_ids = train_ids + val_ids + test_ids\n",
    "\n",
    "# Generate Embeddings (if not exist)\n",
    "# This step generates .h5 files in data/hest_data/cell_embeddings\n",
    "# sCellST expects this structure.\n",
    "\n",
    "TAG = \"imagenet-rn50\"\n",
    "MODEL_NAME = \"resnet50\"\n",
    "NORM_TYPE = \"train\"\n",
    "SHAPE_NAME = \"cellvit\"\n",
    "\n",
    "# Check if embeddings exist to avoid re-running\n",
    "missing_emb = []\n",
    "for sid in all_ids:\n",
    "    emb_path = HEST_DIR / \"cell_embeddings\" / f\"{TAG}_{NORM_TYPE}_{sid}_{SHAPE_NAME}.h5\"\n",
    "    if not emb_path.exists():\n",
    "        missing_emb.append(sid)\n",
    "\n",
    "if missing_emb:\n",
    "    print(f\"Generating embeddings for {missing_emb}...\")\n",
    "    \n",
    "    # 1. Convert to sCellST format (generates cell_images/*.h5)\n",
    "    # This step is required before embed_cells\n",
    "    print(\"Step 1: Converting HEST data to sCellST format...\")\n",
    "    try:\n",
    "        from scellst.cellhest_adapter.processing_utils import convert_to_cellst, filter_data\n",
    "        \n",
    "        # We need to pass technology list. Assuming Visium for these INT datasets.\n",
    "        # Ideally we get this from HEST metadata but let's be explicit if we know.\n",
    "        # Or we can query HEST metadata dataframe as sCellST does.\n",
    "        df = pd.read_csv(ROOT / \"third_party/HEST/assets/HEST_v1_1_0.csv\")\n",
    "        df = df.set_index(\"id\")\n",
    "        tech_list = []\n",
    "        for sid in missing_emb:\n",
    "            if sid in df.index:\n",
    "                tech_list.append(df.loc[sid, \"st_technology\"])\n",
    "            else:\n",
    "                 # Fallback default\n",
    "                 print(f\"Warning: {sid} not found in HEST metadata csv, assuming Visium.\")\n",
    "                 tech_list.append(\"Visium\")\n",
    "        \n",
    "        convert_to_cellst(\n",
    "            path_dataset=HEST_DIR,\n",
    "            ids_to_query=missing_emb,\n",
    "            technology=tech_list,\n",
    "            shape_name=SHAPE_NAME\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error in convert_to_cellst: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # 2. Embed Cells\n",
    "    print(\"Step 2: Generating Embeddings...\")\n",
    "    try:\n",
    "        embed_cells(\n",
    "            path_dataset=HEST_DIR,\n",
    "            organ=None,\n",
    "            ids_to_query=missing_emb,\n",
    "            tag=TAG,\n",
    "            model_name=MODEL_NAME,\n",
    "            normalisation_type=NORM_TYPE,\n",
    "            shape_name=SHAPE_NAME\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Detailed error printing\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(f\"Error generating embeddings: {e}\")\n",
    "        print(\"\\nPotential Issues:\")\n",
    "        print(\"1. Missing source images/h5ad in hest_data?\")\n",
    "        print(f\"   Checking {HEST_DIR} content:\")\n",
    "        if HEST_DIR.exists():\n",
    "            print(f\"   - {len(list(HEST_DIR.glob('*')))} files/dirs found.\")\n",
    "        else:\n",
    "            print(f\"   - {HEST_DIR} NOT FOUND!\")\n",
    "else:\n",
    "    print(\"All embeddings found. Skipping generation.\")"
   ],
   "id": "9e5d7aefe23cdeff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 17512 common genes.\n",
      "All embeddings found. Skipping generation.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training\n",
    "\n",
    "Train using `INT25`, `INT26`. Validate on `INT27`."
   ],
   "id": "c671015f351ddeb3"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T11:20:58.296528600Z",
     "start_time": "2026-01-20T11:20:48.397921600Z"
    }
   },
   "source": [
    "# Load Default Config\n",
    "config_path = SCELLST_DIR / \"config\" / \"gene_default.yaml\"\n",
    "config = load_config(config_path)\n",
    "\n",
    "# Override Config for Training - Following OFFICIAL sCellST settings\n",
    "config.data.data_dir = str(HEST_DIR)\n",
    "# Official approach: Include all training slides in list_training_ids\n",
    "# The DataModule will automatically split them 80/20 for train/val\n",
    "config.data.list_training_ids = train_ids + val_ids  # [INT25, INT26, INT27]\n",
    "config.data.genes = common_genes\n",
    "config.data.embedding_tag = f\"{TAG}_{NORM_TYPE}\"\n",
    "config.data.batch_size = 64  # Official uses 128, reduced for GPU memory\n",
    "config.data.frac_train = 0.8  # Official default - auto split\n",
    "config.data.dataset_handler = \"mil\"\n",
    "# task_type: Use 'regression' for MSE loss, or 'nb_mean_regression' for NB distribution\n",
    "# Official model.yaml uses 'regression'\n",
    "config.model.task_type = \"regression\"\n",
    "\n",
    "# Setup output dir\n",
    "save_dir = RESULT_DIR / \"checkpoints\"\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "best_model_path = save_dir / \"best_model.ckpt\"\n",
    "\n",
    "seed_everything(config.data.seed)\n",
    "\n",
    "# Single DataModule with internal train/val split (OFFICIAL approach)\n",
    "print(\"Setting up DataModule (Train + Val internal split)...\")\n",
    "from scellst.dataset.data_module import STDataModule\n",
    "from scellst.dataset.data_handler import MilVisiumHandler\n",
    "\n",
    "data_dm = STDataModule(\n",
    "    data_dir=Path(config.data.data_dir),\n",
    "    list_training_ids=config.data.list_training_ids,\n",
    "    genes=config.data.genes,\n",
    "    embedding_tag=config.data.embedding_tag,\n",
    "    dataset_handler=MilVisiumHandler(),\n",
    "    batch_size=config.data.batch_size,\n",
    "    frac_train=config.data.frac_train,  # 0.8 - auto 80/20 split\n",
    "    fold=-1,  # Use all slides\n",
    "    predict_id=None,\n",
    "    num_workers=0,  # Windows compatibility\n",
    ")\n",
    "data_dm.prepare_data()\n",
    "data_dm.setup(stage=\"fit\")\n",
    "\n",
    "print(f\"Train samples: {len(data_dm.train_dataset)}, Val samples: {len(data_dm.val_dataset)}\")\n",
    "\n",
    "# Model\n",
    "config.model.gene_names = data_dm.get_gene_names()\n",
    "config.predictor.output_dim = len(config.model.gene_names)\n",
    "model = prepare_model(config)\n",
    "\n",
    "# Check if existing checkpoint exists - LOAD instead of retrain\n",
    "if best_model_path.exists():\n",
    "    print(f\"Found existing checkpoint at {best_model_path}\")\n",
    "    print(\"Loading pre-trained model (to retrain, delete the checkpoint file first)...\")\n",
    "    # Load weights from checkpoint\n",
    "    model = type(model).load_from_checkpoint(best_model_path, weights_only=False)\n",
    "    ckpt_path = str(best_model_path)\n",
    "    print(f\"Model loaded successfully from: {ckpt_path}\")\n",
    "else:\n",
    "    print(\"No existing checkpoint found. Starting training...\")\n",
    "    \n",
    "    # Trainer with Progress Bar (NO TensorBoard/WandB)\n",
    "    from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping, RichProgressBar\n",
    "    from lightning.pytorch.loggers import CSVLogger\n",
    "    import lightning.pytorch.callbacks as pl_callbacks\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=save_dir,\n",
    "        filename='best_model',\n",
    "        save_top_k=1,\n",
    "        monitor='val_loss',\n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    # Official settings: patience=20, max_epochs=400\n",
    "    es_callback = EarlyStopping(monitor='val_loss', patience=20, mode='min')\n",
    "    \n",
    "    # Custom callback: Print one line per epoch for user feedback\n",
    "    class SimpleEpochProgress(pl_callbacks.Callback):\n",
    "        def on_train_epoch_end(self, trainer, pl_module):\n",
    "            metrics = trainer.callback_metrics\n",
    "            loss = metrics.get(\"val_loss\", float('nan'))\n",
    "            train_loss = metrics.get(\"train_loss\", float('nan'))\n",
    "            # Only print if val_loss is available (i.e., after validation runs)\n",
    "            if not isinstance(loss, float) or loss != float('nan'):\n",
    "                print(f\"Epoch {trainer.current_epoch}: train_loss={float(train_loss):.4f}, val_loss={float(loss):.4f}\")\n",
    "    \n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=400,  # Official default\n",
    "        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "        devices=1,\n",
    "        callbacks=[\n",
    "            checkpoint_callback,\n",
    "            es_callback,\n",
    "            RichProgressBar(),\n",
    "            SimpleEpochProgress()\n",
    "        ],\n",
    "        default_root_dir=save_dir,\n",
    "        enable_progress_bar=True,\n",
    "        logger=CSVLogger(save_dir),\n",
    "        log_every_n_steps=10,\n",
    "    )\n",
    "    \n",
    "    print(\"Training with OFFICIAL settings: max_epochs=400, patience=20...\")\n",
    "    # Train using OFFICIAL approach: train_dataloader + val_dataloader from SAME DataModule\n",
    "    trainer.fit(\n",
    "        model,\n",
    "        train_dataloaders=data_dm.train_dataloader(),\n",
    "        val_dataloaders=data_dm.val_dataloader()\n",
    "    )\n",
    "    ckpt_path = checkpoint_callback.best_model_path\n",
    "    print(f\"Training finished. Best model saved to: {ckpt_path}\")"
   ],
   "id": "c3eb179708dbe587",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up DataModule (Train + Val internal split)...\n",
      "\u001B[32m19:20:49\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mPreparing data for training.\u001B[0m\n",
      "\u001B[32m19:20:49\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mPreparing data for ID: INT25.\u001B[0m\n",
      "\u001B[32m19:20:49\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mLoading data for ID: INT25\u001B[0m\n",
      "\u001B[32m19:20:49\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mLoaded adata with shape: (3808, 17943)\u001B[0m\n",
      "\u001B[32m19:20:49\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mFirst obs names: Index(['AAACAAGTATCTCCCA-1_INT25', 'AAACACCAATAACTGC-1_INT25',\n",
      "       'AAACAGAGCGACTCCT-1_INT25', 'AAACAGGGTCTATATT-1_INT25',\n",
      "       'AAACATTTCCCGGATT-1_INT25'],\n",
      "      dtype='object')\u001B[0m\n",
      "\u001B[32m19:20:49\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mAfter genes filtering: (3808, 9795)\u001B[0m\n",
      "\u001B[32m19:20:49\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mAfter count filtering: (3804, 9795)\u001B[0m\n",
      "\u001B[32m19:20:49\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mAfter mt and rps filtering: 9795\u001B[0m\n",
      "\u001B[32m19:20:49\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mNormalising spot counts.\u001B[0m\n",
      "\u001B[32m19:20:49\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mLog1p transform counts.\u001B[0m\n",
      "\u001B[32m19:20:50\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mFound 3440 / 3804 spots in the adata file.\u001B[0m\n",
      "\u001B[32m19:20:50\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mFound 3440 / 3440 spots in the cell embedding file.\u001B[0m\n",
      "\u001B[32m19:20:50\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mPreprocessing completed.\u001B[0m\n",
      "\u001B[32m19:20:50\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mSelecting genes from list.\u001B[0m\n",
      "\u001B[32m19:20:50\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mFound 9586 / 17512 genes in adata.var_names\u001B[0m\n",
      "\u001B[32m19:20:50\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mPreparing data for ID: INT26.\u001B[0m\n",
      "\u001B[32m19:20:50\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mLoading data for ID: INT26\u001B[0m\n",
      "\u001B[32m19:20:50\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mLoaded adata with shape: (3894, 17943)\u001B[0m\n",
      "\u001B[32m19:20:50\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mFirst obs names: Index(['AAACAAGTATCTCCCA-1_INT26', 'AAACACCAATAACTGC-1_INT26',\n",
      "       'AAACAGAGCGACTCCT-1_INT26', 'AAACAGCTTTCAGAAG-1_INT26',\n",
      "       'AAACAGGGTCTATATT-1_INT26'],\n",
      "      dtype='object')\u001B[0m\n",
      "\u001B[32m19:20:51\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mAfter genes filtering: (3894, 9492)\u001B[0m\n",
      "\u001B[32m19:20:51\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mAfter count filtering: (3894, 9492)\u001B[0m\n",
      "\u001B[32m19:20:51\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mAfter mt and rps filtering: 9492\u001B[0m\n",
      "\u001B[32m19:20:51\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mNormalising spot counts.\u001B[0m\n",
      "\u001B[32m19:20:51\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mLog1p transform counts.\u001B[0m\n",
      "\u001B[32m19:20:51\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mFound 3711 / 3894 spots in the adata file.\u001B[0m\n",
      "\u001B[32m19:20:51\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mFound 3711 / 3711 spots in the cell embedding file.\u001B[0m\n",
      "\u001B[32m19:20:51\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mPreprocessing completed.\u001B[0m\n",
      "\u001B[32m19:20:51\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mSelecting genes from list.\u001B[0m\n",
      "\u001B[32m19:20:51\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mFound 9294 / 17512 genes in adata.var_names\u001B[0m\n",
      "\u001B[32m19:20:52\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mPreparing data for ID: INT27.\u001B[0m\n",
      "\u001B[32m19:20:52\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mLoading data for ID: INT27\u001B[0m\n",
      "\u001B[32m19:20:52\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mLoaded adata with shape: (4176, 17943)\u001B[0m\n",
      "\u001B[32m19:20:52\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mFirst obs names: Index(['AAACAAGTATCTCCCA-1_INT27', 'AAACACCAATAACTGC-1_INT27',\n",
      "       'AAACAGAGCGACTCCT-1_INT27', 'AAACAGCTTTCAGAAG-1_INT27',\n",
      "       'AAACAGGGTCTATATT-1_INT27'],\n",
      "      dtype='object')\u001B[0m\n",
      "\u001B[32m19:20:52\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mAfter genes filtering: (4176, 11133)\u001B[0m\n",
      "\u001B[32m19:20:52\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mAfter count filtering: (4176, 11133)\u001B[0m\n",
      "\u001B[32m19:20:52\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mAfter mt and rps filtering: 11133\u001B[0m\n",
      "\u001B[32m19:20:52\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mNormalising spot counts.\u001B[0m\n",
      "\u001B[32m19:20:52\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mLog1p transform counts.\u001B[0m\n",
      "\u001B[32m19:20:53\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mFound 3777 / 4176 spots in the adata file.\u001B[0m\n",
      "\u001B[32m19:20:53\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mFound 3777 / 3777 spots in the cell embedding file.\u001B[0m\n",
      "\u001B[32m19:20:53\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mPreprocessing completed.\u001B[0m\n",
      "\u001B[32m19:20:53\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mSelecting genes from list.\u001B[0m\n",
      "\u001B[32m19:20:53\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mFound 10905 / 17512 genes in adata.var_names\u001B[0m\n",
      "\u001B[32m19:20:54\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mComputing the sorted intersection of genes across all slides.\u001B[0m\n",
      "\u001B[32m19:20:55\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mSelected 9204 common genes across all slides.\u001B[0m\n",
      "\u001B[32m19:20:55\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mData preparation completed.\u001B[0m\n",
      "\u001B[32m19:20:55\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mUsing all training slides: ['INT25', 'INT26', 'INT27'].\u001B[0m\n",
      "\u001B[32m19:20:55\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mNo scaling applied\u001B[0m\n",
      "\u001B[32m19:20:56\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mLoading cell embeddings from D:\\code\\Morpho-VC\\data\\hest_data\\cell_embeddings\\imagenet-rn50_train_INT25_cellvit.h5\u001B[0m\n",
      "\u001B[32m19:20:56\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mLoading cell embeddings from D:\\code\\Morpho-VC\\data\\hest_data\\cell_embeddings\\imagenet-rn50_train_INT26_cellvit.h5\u001B[0m\n",
      "\u001B[32m19:20:56\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mLoading cell embeddings from D:\\code\\Morpho-VC\\data\\hest_data\\cell_embeddings\\imagenet-rn50_train_INT27_cellvit.h5\u001B[0m\n",
      "\u001B[32m19:20:56\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mUsing 8743 datapoint for training and 2185 for validation.\u001B[0m\n",
      "Train samples: 8743, Val samples: 2185\n",
      "\u001B[32m19:20:56\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mUsing input dimension: 2048\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\scellst_bench\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: Metric `SpearmanCorrcoef` will save all targets and predictions in the buffer. For large datasets, this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m19:20:56\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mGeneLightningModel(\n",
      "  (train_metrics): MetricCollection(\n",
      "    (mse): MeanSquaredError()\n",
      "    (pcc): PearsonCorrCoef(),\n",
      "    prefix=val/\n",
      "  )\n",
      "  (valid_metrics): MetricCollection(\n",
      "    (mse): MeanSquaredError()\n",
      "    (pcc): PearsonCorrCoef(),\n",
      "    prefix=val/\n",
      "  )\n",
      "  (test_metrics): MetricCollection(\n",
      "    (mse): MeanSquaredError()\n",
      "    (pcc): PearsonCorrCoef()\n",
      "    (scc): SpearmanCorrCoef()\n",
      "  )\n",
      "  (model): InstanceMilModel(\n",
      "    (gene_predictor): GenePredictor(\n",
      "      (final_activation_layer): Softplus(beta=20, threshold=20.0)\n",
      "      (model): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=9204, bias=True)\n",
      "          (1): Softplus(beta=20, threshold=20.0)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\u001B[0m\n",
      "Found existing checkpoint at D:\\code\\Morpho-VC\\benchmark\\results\\sCellST\\checkpoints\\best_model.ckpt\n",
      "Loading pre-trained model (to retrain, delete the checkpoint file first)...\n",
      "\u001B[32m19:20:58\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mGeneLightningModel(\n",
      "  (train_metrics): MetricCollection(\n",
      "    (mse): MeanSquaredError()\n",
      "    (pcc): PearsonCorrCoef(),\n",
      "    prefix=val/\n",
      "  )\n",
      "  (valid_metrics): MetricCollection(\n",
      "    (mse): MeanSquaredError()\n",
      "    (pcc): PearsonCorrCoef(),\n",
      "    prefix=val/\n",
      "  )\n",
      "  (test_metrics): MetricCollection(\n",
      "    (mse): MeanSquaredError()\n",
      "    (pcc): PearsonCorrCoef()\n",
      "    (scc): SpearmanCorrCoef()\n",
      "  )\n",
      "  (model): InstanceMilModel(\n",
      "    (gene_predictor): GenePredictor(\n",
      "      (final_activation_layer): Softplus(beta=20, threshold=20.0)\n",
      "      (model): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=9204, bias=True)\n",
      "          (1): Softplus(beta=20, threshold=20.0)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\u001B[0m\n",
      "Model loaded successfully from: D:\\code\\Morpho-VC\\benchmark\\results\\sCellST\\checkpoints\\best_model.ckpt\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prediction (INT28)\n",
    "\n",
    "Load best model and predict on `INT28`."
   ],
   "id": "50f7d6acfe50eb9f"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T11:21:04.059001700Z",
     "start_time": "2026-01-20T11:20:58.374435900Z"
    }
   },
   "source": [
    "# --- Manual Prediction Setup ---\n",
    "print(f\"Loading model from {ckpt_path}\")\n",
    "# FIX: Set weights_only=False to fix UnpicklingError with DictConfig\n",
    "model = type(model).load_from_checkpoint(ckpt_path, weights_only=False)\n",
    "model.eval()\n",
    "model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Retrieve the exact gene list used during training (from model hparams)\n",
    "train_gene_names = model.gene_names\n",
    "print(f\"Model expects {len(train_gene_names)} genes.\")\n",
    "\n",
    "# Manually load and align data for prediction\n",
    "predict_id = test_ids[0]\n",
    "print(f\"Manually preparing prediction data for ID: {predict_id}\")\n",
    "\n",
    "handler = MilVisiumHandler()\n",
    "embedding_path = Path(config.data.data_dir) / \"cell_embeddings\" / f\"{config.data.embedding_tag}_{predict_id}_{config.data.shape_name}.h5\"\n",
    "\n",
    "# Load adata - this sets adata.uns[\"cell_embedding_path\"] and adata.uns[\"spot_cell_map\"]\n",
    "adata = handler.load_and_preprocess_data(\n",
    "    data_dir=Path(config.data.data_dir),\n",
    "    id=predict_id,\n",
    "    filter_genes=config.data.filter_genes,\n",
    "    filter_cells=config.data.filter_cells,\n",
    "    normalize=config.data.normalize,\n",
    "    log1p=config.data.log1p,\n",
    "    embedding_path=embedding_path,\n",
    "    shape_name=config.data.shape_name,\n",
    ")\n",
    "\n",
    "# Convert sparse to dense if needed (same as STDataModule)\n",
    "from scipy.sparse import issparse\n",
    "if issparse(adata.X):\n",
    "    adata.X = adata.X.toarray()\n",
    "\n",
    "# --- Gene Alignment Logic ---\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import scipy.sparse\n",
    "import numpy as np\n",
    "\n",
    "# CRITICAL: Backup ALL metadata before any transformation\n",
    "# EmbeddedMilDataset requires both of these\n",
    "saved_uns = adata.uns.copy()\n",
    "saved_obsm = adata.obsm.copy() if hasattr(adata, 'obsm') and len(adata.obsm) > 0 else {}\n",
    "saved_obs = adata.obs.copy()\n",
    "\n",
    "# 1. Filter out genes not in training set (common genes)\n",
    "common = sorted(list(set(adata.var_names).intersection(train_gene_names)))\n",
    "adata = adata[:, common].copy()\n",
    "\n",
    "# 2. Identify missing genes\n",
    "missing_genes = sorted(list(set(train_gene_names) - set(adata.var_names)))\n",
    "\n",
    "if len(missing_genes) > 0:\n",
    "    print(f\"Warning: Prediction set is missing {len(missing_genes)} genes. Filling with zeros.\")\n",
    "    \n",
    "    # Prepare zero-filled matrix for missing genes\n",
    "    zero_X = np.zeros((adata.n_obs, len(missing_genes)), dtype=adata.X.dtype)\n",
    "    \n",
    "    # Concatenate horizontally\n",
    "    new_X = np.hstack([adata.X, zero_X])\n",
    "    new_var_names = list(adata.var_names) + missing_genes\n",
    "    \n",
    "    # Create new AnnData with combined data\n",
    "    adata = ad.AnnData(\n",
    "        X=new_X,\n",
    "        obs=saved_obs,  # Use saved obs\n",
    "        var=pd.DataFrame(index=new_var_names),\n",
    "    )\n",
    "\n",
    "# CRITICAL: Restore metadata after transformation\n",
    "adata.uns = saved_uns\n",
    "if saved_obsm:\n",
    "    adata.obsm = saved_obsm\n",
    "\n",
    "# 3. Reorder strictly to match training genes order\n",
    "# AND backup/restore uns again because slicing can drop it\n",
    "saved_uns_final = adata.uns.copy()\n",
    "adata = adata[:, train_gene_names].copy()\n",
    "adata.uns = saved_uns_final\n",
    "\n",
    "print(f\"Aligned adata shape: {adata.shape}\")\n",
    "print(f\"adata.uns keys: {list(adata.uns.keys())}\")\n",
    "\n",
    "# Create Dataset manually\n",
    "# MilVisiumHandler.create_dataset expects adata with:\n",
    "# - adata.uns[\"cell_embedding_path\"]\n",
    "# - adata.uns[\"spot_cell_map\"] \n",
    "# - adata.obs[\"size_factor\"]\n",
    "predict_ds = handler.create_dataset(adata, embedding_path)\n",
    "\n",
    "# Create DataLoader with custom_collate (required for MIL)\n",
    "from torch.utils.data import DataLoader\n",
    "from scellst.dataset.dataset_utils import custom_collate\n",
    "from scellst.constant import REGISTRY_KEYS\n",
    "\n",
    "loader = DataLoader(\n",
    "    predict_ds, \n",
    "    batch_size=config.data.batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=0,\n",
    "    collate_fn=custom_collate\n",
    ")\n",
    "\n",
    "# Prediction Loop\n",
    "preds = []\n",
    "truths = []\n",
    "\n",
    "print(f\"Predicting on {predict_id}...\")\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(loader, desc=\"Predicting\", file=sys.stdout):\n",
    "        batch_to_device = {}\n",
    "        for k, v in batch.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                batch_to_device[k] = v.to(model.device)\n",
    "            else:\n",
    "                batch_to_device[k] = v\n",
    "        \n",
    "        # Call internal model forward (model.model is InstanceDistributionMilModel)\n",
    "        bag_dict, _ = model.model(batch_to_device)\n",
    "        \n",
    "        # CORRECT KEY: REGISTRY_KEYS.OUTPUT_PREDICTION = \"output_prediction\"\n",
    "        pred = bag_dict[REGISTRY_KEYS.OUTPUT_PREDICTION].cpu().numpy()\n",
    "        \n",
    "        # CORRECT KEY: REGISTRY_KEYS.Y_BAG_KEY = \"Y_bag\"\n",
    "        truth = batch[REGISTRY_KEYS.Y_BAG_KEY].cpu().numpy()\n",
    "        \n",
    "        preds.append(pred)\n",
    "        truths.append(truth)\n",
    "\n",
    "pred_bag = np.concatenate(preds, axis=0)\n",
    "true_bag = np.concatenate(truths, axis=0)\n",
    "\n",
    "print(f\"Prediction shape: {pred_bag.shape}\")\n",
    "print(f\"Truth shape: {true_bag.shape}\")\n",
    "\n",
    "# Save Results\n",
    "np.save(RESULT_DIR / 'pred_bag.npy', pred_bag)\n",
    "np.save(RESULT_DIR / 'true_bag.npy', true_bag)\n",
    "print(f\"Saved results to {RESULT_DIR}\")"
   ],
   "id": "fb19bb92a82c90b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from D:\\code\\Morpho-VC\\benchmark\\results\\sCellST\\checkpoints\\best_model.ckpt\n",
      "\u001B[32m19:20:59\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mGeneLightningModel(\n",
      "  (train_metrics): MetricCollection(\n",
      "    (mse): MeanSquaredError()\n",
      "    (pcc): PearsonCorrCoef(),\n",
      "    prefix=val/\n",
      "  )\n",
      "  (valid_metrics): MetricCollection(\n",
      "    (mse): MeanSquaredError()\n",
      "    (pcc): PearsonCorrCoef(),\n",
      "    prefix=val/\n",
      "  )\n",
      "  (test_metrics): MetricCollection(\n",
      "    (mse): MeanSquaredError()\n",
      "    (pcc): PearsonCorrCoef()\n",
      "    (scc): SpearmanCorrCoef()\n",
      "  )\n",
      "  (model): InstanceMilModel(\n",
      "    (gene_predictor): GenePredictor(\n",
      "      (final_activation_layer): Softplus(beta=20, threshold=20.0)\n",
      "      (model): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=9204, bias=True)\n",
      "          (1): Softplus(beta=20, threshold=20.0)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\u001B[0m\n",
      "Model expects 9204 genes.\n",
      "Manually preparing prediction data for ID: INT28\n",
      "\u001B[32m19:20:59\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mLoading data for ID: INT28\u001B[0m\n",
      "\u001B[32m19:20:59\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mLoaded adata with shape: (3990, 17943)\u001B[0m\n",
      "\u001B[32m19:20:59\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mFirst obs names: Index(['AAACAACGAATAGTTC-1_INT28', 'AAACAAGTATCTCCCA-1_INT28',\n",
      "       'AAACAATCTACTAGCA-1_INT28', 'AAACACCAATAACTGC-1_INT28',\n",
      "       'AAACAGAGCGACTCCT-1_INT28'],\n",
      "      dtype='object')\u001B[0m\n",
      "\u001B[32m19:20:59\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mAfter genes filtering: (3990, 10630)\u001B[0m\n",
      "\u001B[32m19:20:59\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mAfter count filtering: (3990, 10630)\u001B[0m\n",
      "\u001B[32m19:20:59\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mAfter mt and rps filtering: 10630\u001B[0m\n",
      "\u001B[32m19:20:59\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mNormalising spot counts.\u001B[0m\n",
      "\u001B[32m19:20:59\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mLog1p transform counts.\u001B[0m\n",
      "\u001B[32m19:21:00\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mFound 3849 / 3990 spots in the adata file.\u001B[0m\n",
      "\u001B[32m19:21:00\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mFound 3849 / 3849 spots in the cell embedding file.\u001B[0m\n",
      "\u001B[32m19:21:00\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mPreprocessing completed.\u001B[0m\n",
      "Warning: Prediction set is missing 70 genes. Filling with zeros.\n",
      "Aligned adata shape: (3849, 9204)\n",
      "adata.uns keys: ['spatial', 'hest_id', 'log1p', 'cell_embedding_path', 'spot_cell_map']\n",
      "\u001B[32m19:21:01\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[1mLoading cell embeddings from D:\\code\\Morpho-VC\\data\\hest_data\\cell_embeddings\\imagenet-rn50_train_INT28_cellvit.h5\u001B[0m\n",
      "Predicting on INT28...\n",
      "Predicting: 100%|██████████| 61/61 [00:02<00:00, 24.73it/s]\n",
      "Prediction shape: (3849, 9204)\n",
      "Truth shape: (3849, 9204)\n",
      "Saved results to D:\\code\\Morpho-VC\\benchmark\\results\\sCellST\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    "\n",
    "Calculate MAE, RMSE, Pearson Correlation."
   ],
   "id": "7f766882e054c58b"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T11:21:05.424940100Z",
     "start_time": "2026-01-20T11:21:04.152032200Z"
    }
   },
   "source": [
    "def pearson_corr(a, b):\n",
    "    if np.all(a == a[0]) or np.all(b == b[0]):\n",
    "        return np.nan\n",
    "    a = a - a.mean()\n",
    "    b = b - b.mean()\n",
    "    denom = np.sqrt((a * a).sum()) * np.sqrt((b * b).sum())\n",
    "    if denom == 0:\n",
    "        return np.nan\n",
    "    return float((a * b).sum() / denom)\n",
    "\n",
    "mae = np.mean(np.abs(pred_bag - true_bag))\n",
    "rmse = np.sqrt(np.mean((pred_bag - true_bag) ** 2))\n",
    "\n",
    "# Gene-wise Pearson\n",
    "gene_corrs = []\n",
    "for i in range(pred_bag.shape[1]):\n",
    "    corr = pearson_corr(pred_bag[:, i], true_bag[:, i])\n",
    "    gene_corrs.append(corr)\n",
    "\n",
    "valid = [(i, c) for i, c in enumerate(gene_corrs) if not np.isnan(c)]\n",
    "mean_gene_corr = float(np.mean([c for _, c in valid])) if valid else float('nan')\n",
    "\n",
    "best_gene = max(valid, key=lambda x: x[1]) if valid else (None, None)\n",
    "best_gene_name = common_genes[best_gene[0]] if best_gene[0] is not None else \"NA\"\n",
    "\n",
    "print(f'MAE: {mae:.4f}')\n",
    "print(f'RMSE: {rmse:.4f}')\n",
    "print(f'Mean Pearson: {mean_gene_corr:.4f}')\n",
    "print(f'Best Gene: {best_gene_name} ({best_gene[1]:.4f})')\n",
    "\n",
    "metrics = {\n",
    "    'MAE': float(mae),\n",
    "    'RMSE': float(rmse),\n",
    "    'Mean_Pearson': float(mean_gene_corr),\n",
    "    'Best_Gene': best_gene_name,\n",
    "    'Best_Pearson': float(best_gene[1]) if best_gene[1] is not None else None\n",
    "}\n",
    "with open(RESULT_DIR / 'metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)"
   ],
   "id": "7585f11730a0cfff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.3363\n",
      "RMSE: 0.4483\n",
      "Mean Pearson: 0.0779\n",
      "Best Gene: LMX1B (0.6885)\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
