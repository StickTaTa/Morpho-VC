{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iStar Benchmark (INT25-INT28)\n",
    "\n",
    "## 环境配置说明\n",
    "iStar 依赖于特定的 PyTorch 版本和库。建议创建一个新的 Conda 环境。\n",
    "\n",
    "### 1. 创建并激活新环境\n",
    "```bash\n",
    "conda create -n istar_bench python=3.9\n",
    "conda activate istar_bench\n",
    "```\n",
    "\n",
    "### 2. 安装依赖\n",
    "```bash\n",
    "# 安装 PyTorch 2.0.1 (根据 CUDA 版本调整，这里示例为 CUDA 11.8)\n",
    "pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# 安装其他依赖\n",
    "pip install numpy==1.25.2 pillow==10.0.0 pandas==2.1.0 scikit-image==0.21.0 opencv-python==4.8.0.76 einops==0.6.1 tomli==2.0.1 pytorch-lightning==2.0.8 matplotlib==3.7.2 scikit-learn==1.3.1 umap-learn==0.5.5 seaborn==0.13.1 scanpy scprep tqdm openslide-python\n",
    "```\n",
    "\n",
    "**注意**：同样需要安装 OpenSlide 二进制文件并配置 PATH。\n",
    "\n",
    "---\n",
    "\n",
    "## 任务描述\n",
    "对比 Morpho-VC 与 iStar 在 INT25-INT28 数据集上的表现。\n",
    "\n",
    "**流程**：\n",
    "1. **数据准备**：将 HEST 数据转换为 iStar 格式（图像 + 表达矩阵 + 坐标）。\n",
    "2. **预处理**：运行 iStar 的预处理和特征提取脚本。\n",
    "3. **训练**：在 INT25 上训练，保存模型；接着在 INT26 上微调（模拟多切片训练）。\n",
    "4. **预测**：在 INT28 上预测。\n",
    "5. **评估**：计算 MAE, RMSE, Pearson。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T06:22:40.454340300Z",
     "start_time": "2026-01-19T06:22:38.356594500Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import scanpy as sc\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# OpenSlide Fix for Windows\n",
    "if os.name == 'nt':\n",
    "    conda_prefix = os.environ.get('CONDA_PREFIX')\n",
    "    curr_python = sys.executable\n",
    "    possible_paths = []\n",
    "    if conda_prefix:\n",
    "        possible_paths.append(Path(conda_prefix) / 'Library' / 'bin')\n",
    "    if curr_python:\n",
    "        possible_paths.append(Path(curr_python).parent / 'Library' / 'bin')\n",
    "        possible_paths.append(Path(curr_python).parent / '..' / 'Library' / 'bin')\n",
    "    for p in possible_paths:\n",
    "        if p.exists() and ((p / 'libopenslide-1.dll').exists() or (p / 'libopenslide-0.dll').exists()):\n",
    "            print(f\"Found OpenSlide DLL at: {p}\")\n",
    "            try:\n",
    "                os.add_dll_directory(str(p))\n",
    "            except AttributeError:\n",
    "                pass\n",
    "            os.environ['PATH'] = str(p) + os.pathsep + os.environ['PATH']\n",
    "            break\n",
    "\n",
    "try:\n",
    "    import openslide\n",
    "except ImportError:\n",
    "    print(\"OpenSlide not found. ROI extraction might fail if not using PIL fallback correctly.\")\n",
    "\n",
    "# 路径设置\n",
    "ROOT = Path(os.environ.get('MORPHO_VC_ROOT', '../')).expanduser().resolve()\n",
    "print(f\"ROOT: {ROOT}\")\n",
    "\n",
    "sys.path.append(str(ROOT / 'src'))\n",
    "sys.path.append(str(ROOT / 'benchmark'))\n",
    "\n",
    "from hest_dataset import HESTHisToGeneDataset\n",
    " \n",
    "# iStar 源码路径\n",
    "ISTAR_ROOT = ROOT / 'benchmark' / 'istar'\n",
    "sys.path.append(str(ISTAR_ROOT))\n",
    "\n",
    "# 结果与临时数据目录\n",
    "RESULT_DIR = ROOT / 'benchmark' / 'results' / 'istar'\n",
    "DATA_ROOT = RESULT_DIR / 'data' # iStar 脚本通常在 data/ 目录下找数据\n",
    "RESULT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 数据路径\n",
    "spatial_dir = ROOT / 'data' / 'spatial_data'\n",
    "hest_dir = ROOT / 'data' / 'hest_data'\n",
    "common_gene_path = spatial_dir / 'common_genes.txt'\n",
    "\n",
    "with open(common_gene_path, 'r') as f:\n",
    "    common_genes = f.read().splitlines()\n",
    "\n",
    "print(f\"Common Genes: {len(common_genes)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: D:\\code\\Morpho-VC\n",
      "Common Genes: 17512\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T06:22:40.494968700Z",
     "start_time": "2026-01-19T06:22:40.456364300Z"
    }
   },
   "source": [
    "# Check and download iStar checkpoints\n",
    "import urllib.request\n",
    "\n",
    "# Override SSL context to avoid errors in some envs\n",
    "import ssl\n",
    "try:\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "CHECKPOINT_DIR = ISTAR_ROOT / 'checkpoints'\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "urls = {\n",
    "    \"vit256_small_dino.pth\": \"https://github.com/mahmoodlab/HIPT/raw/master/HIPT_4K/Checkpoints/vit256_small_dino.pth\",\n",
    "    \"vit4k_xs_dino.pth\": \"https://github.com/mahmoodlab/HIPT/raw/master/HIPT_4K/Checkpoints/vit4k_xs_dino.pth\"\n",
    "}\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "for name, url in urls.items():\n",
    "    target_path = CHECKPOINT_DIR / name\n",
    "    if not target_path.exists():\n",
    "        print(f\"Downloading {name}...\")\n",
    "        try:\n",
    "            req = urllib.request.Request(url, headers=headers)\n",
    "            with urllib.request.urlopen(req) as response, open(target_path, 'wb') as out_file:\n",
    "                out_file.write(response.read())\n",
    "            print(\"Done.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {name}: {e}\")\n",
    "    else:\n",
    "        print(f\"{name} exists.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit256_small_dino.pth exists.\n",
      "vit4k_xs_dino.pth exists.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据转换 (HEST -> iStar)\n",
    "\n",
    "iStar 需要每个样本一个文件夹，包含：\n",
    "- `he-raw.jpg`: 高分辨率 H&E 图像\n",
    "- `cnts.tsv`: 基因计数矩阵 (Spots x Genes)\n",
    "- `locs-raw.tsv`: Spot 坐标 (image pixel coordinates)\n",
    "- `pixel-size-raw.txt`: 像素大小 (um)\n",
    "- `radius-raw.txt`: Spot 半径 (pixels)\n",
    "\n",
    "我们将使用 `HESTHisToGeneDataset` 中的逻辑来加载数据，然后保存为上述格式。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T06:22:40.529043800Z",
     "start_time": "2026-01-19T06:22:40.497477600Z"
    }
   },
   "source": [
    "def convert_hest_to_istar(slide_id, output_dir):\n",
    "    print(f\"Processing {slide_id}...\")\n",
    "    sample_dir = output_dir / slide_id\n",
    "    \n",
    "    # Check if exists (robust check)\n",
    "    if sample_dir.exists() and (sample_dir / 'cnts.tsv').exists():\n",
    "        # Check for gene-names.txt\n",
    "        if not (sample_dir / 'gene-names.txt').exists():\n",
    "             print(\"  Injecting missing gene-names.txt...\")\n",
    "             # Load header to get genes\n",
    "             df_head = pd.read_csv(sample_dir / 'cnts.tsv', sep='\\t', index_col=0, nrows=0)\n",
    "             genes = df_head.columns.tolist()\n",
    "             with open(sample_dir / 'gene-names.txt', 'w') as f:\n",
    "                 for g in genes:\n",
    "                     f.write(g + '\\n')\n",
    "        \n",
    "        # Check for pixel-size.txt\n",
    "        if not (sample_dir / 'pixel-size.txt').exists():\n",
    "            print(\"  Injecting missing pixel-size.txt...\")\n",
    "            with open(sample_dir / 'pixel-size.txt', 'w') as f:\n",
    "                f.write('0.5')\n",
    "                \n",
    "        print(f\"  {sample_dir} valid. Skipping conversion.\")\n",
    "        return\n",
    "        \n",
    "    sample_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. Load h5ad\n",
    "    adata_path = spatial_dir / f\"{slide_id}.h5ad\"\n",
    "    adata = sc.read_h5ad(adata_path)\n",
    "    \n",
    "    # Filter Common Genes\n",
    "    adata = adata[:, common_genes].copy()\n",
    "    \n",
    "    # 2. Save counts (cnts.tsv)\n",
    "    # Row 1: Gene names\n",
    "    # Col 1: Spot ID\n",
    "    df_cnts = pd.DataFrame(adata.X.toarray() if scipy.sparse.issparse(adata.X) else adata.X, \n",
    "                           index=adata.obs_names, columns=adata.var_names)\n",
    "    df_cnts.index.name = 'SpotID'\n",
    "    df_cnts.reset_index(inplace=True)\n",
    "    df_cnts.to_csv(sample_dir / 'cnts.tsv', sep='\\t', index=False)\n",
    "    \n",
    "    # Save gene-names.txt (Required by iStar impute.py)\n",
    "    with open(sample_dir / 'gene-names.txt', 'w') as f:\n",
    "        for g in adata.var_names:\n",
    "            f.write(g + '\\n')\n",
    "    \n",
    "    # 3. Process Image & Coordinates\n",
    "    # Read WSI path\n",
    "    tiff_path = hest_dir / 'wsis' / f\"{slide_id}.tif\"\n",
    "    if not tiff_path.exists():\n",
    "        tiff_path = hest_dir / 'wsis' / f\"{slide_id}.svs\"\n",
    "    \n",
    "    if not tiff_path.exists():\n",
    "        raise FileNotFoundError(f\"WSI not found for {slide_id}\")\n",
    "        \n",
    "    slide = openslide.OpenSlide(str(tiff_path))\n",
    "    w, h = slide.dimensions\n",
    "    \n",
    "    # Target Max Size\n",
    "    MAX_SIZE = 4000\n",
    "    scale = min(MAX_SIZE / w, MAX_SIZE / h)\n",
    "    \n",
    "    # Calculate level\n",
    "    level = slide.get_best_level_for_downsample(1/scale)\n",
    "    img = slide.read_region((0, 0), level, slide.level_dimensions[level])\n",
    "    img = img.convert('RGB')\n",
    "    \n",
    "    # True scale\n",
    "    true_scale_w = img.size[0] / w\n",
    "    true_scale_h = img.size[1] / h\n",
    "    \n",
    "    # Save Image\n",
    "    img.save(sample_dir / 'he-raw.jpg')\n",
    "    \n",
    "    # 4. Save Coordinates (locs-raw.tsv)\n",
    "    coords = adata.obsm['spatial'].copy().astype(float)\n",
    "    coords[:, 0] *= true_scale_w\n",
    "    coords[:, 1] *= true_scale_h\n",
    "    \n",
    "    df_locs = pd.DataFrame(coords, index=adata.obs_names, columns=['x', 'y'])\n",
    "    df_locs.index.name = 'SpotID'\n",
    "    df_locs.reset_index(inplace=True)\n",
    "    df_locs.to_csv(sample_dir / 'locs-raw.tsv', sep='\\t', index=False)\n",
    "    \n",
    "    # 5. Pixel Size & Radius\n",
    "    # HEST slides usually ~0.5 um/px.\n",
    "    original_mpp = 0.5 \n",
    "    current_pixel_size = original_mpp / true_scale_w\n",
    "    \n",
    "    with open(sample_dir / 'pixel-size-raw.txt', 'w') as f:\n",
    "        f.write(str(current_pixel_size))\n",
    "        \n",
    "    # Radius ~50um\n",
    "    radius_px = 50.0 / current_pixel_size\n",
    "    with open(sample_dir / 'radius-raw.txt', 'w') as f:\n",
    "        f.write(str(radius_px))\n",
    "        \n",
    "    # Create pixel-size.txt for rescale.py\n",
    "    with open(sample_dir / 'pixel-size.txt', 'w') as f:\n",
    "        f.write('0.5')\n",
    "        \n",
    "    print(f\"  Saved to {sample_dir}\")\n",
    "    print(f\"  Image Size: {img.size}, Scale: {true_scale_w:.4f}, Pixel Size: {current_pixel_size:.2f}, Radius: {radius_px:.2f}\")\n",
    "\n",
    "import scipy.sparse\n",
    "\n",
    "# Convert all slides\n",
    "ids = ['INT25', 'INT26', 'INT27', 'INT28']\n",
    "for s_id in ids:\n",
    "    convert_hest_to_istar(s_id, DATA_ROOT)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing INT25...\n",
      "  D:\\code\\Morpho-VC\\benchmark\\results\\istar\\data\\INT25 valid. Skipping conversion.\n",
      "Processing INT26...\n",
      "  D:\\code\\Morpho-VC\\benchmark\\results\\istar\\data\\INT26 valid. Skipping conversion.\n",
      "Processing INT27...\n",
      "  D:\\code\\Morpho-VC\\benchmark\\results\\istar\\data\\INT27 valid. Skipping conversion.\n",
      "Processing INT28...\n",
      "  D:\\code\\Morpho-VC\\benchmark\\results\\istar\\data\\INT28 valid. Skipping conversion.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 运行 iStar 预处理\n",
    "\n",
    "对每个样本运行特征提取。这可能需要一些时间。\n",
    "iStar 脚本路径：`benchmark/istar/*.py`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T06:22:40.556604600Z",
     "start_time": "2026-01-19T06:22:40.530044400Z"
    }
   },
   "source": [
    "def run_istar_preprocess(slide_id):\n",
    "    print(f\"Preprocessing {slide_id}...\")\n",
    "    prefix = f\"results/istar/data/{slide_id}/\"\n",
    "    # 注意：我们要在 istar 目录下运行命令，以便它能找到其他模块\n",
    "    # 或者设置 PYTHONPATH。简便起见，我们在 ISTAR_ROOT 下运行，并传递相对路径。\n",
    "    \n",
    "    # 相对路径计算\n",
    "    # ISTAR_ROOT is d:\\code\\Morpho-VC\\benchmark\\istar\n",
    "    # Data is d:\\code\\Morpho-VC\\benchmark\\results\\istar\\data\\{slide_id}\n",
    "    # Relative path from ISTAR_ROOT to Data\n",
    "    rel_prefix = os.path.relpath(DATA_ROOT / slide_id, ISTAR_ROOT).replace('\\\\', '/') + '/'\n",
    "    \n",
    "    cmds = [\n",
    "        # 1. Rescale (Scale image to desired pixel size, typically 0.5um)\n",
    "        # 我们在转换时已经计算了 pixel-size-raw.txt\n",
    "        f\"python rescale.py {rel_prefix} --image\",\n",
    "        \n",
    "        # 2. Preprocess (Tiling, etc.)\n",
    "        f\"python preprocess.py {rel_prefix} --image\",\n",
    "        \n",
    "        # 3. Extract Features (CNN + ViT features)\n",
    "        f\"python extract_features.py {rel_prefix} --device=cuda\",\n",
    "        \n",
    "        # 4. Get Mask\n",
    "        f\"python get_mask.py {rel_prefix}embeddings-hist.pickle {rel_prefix}mask-small.png\",\n",
    "        \n",
    "        # 5. Rescale locs & radius for prediction map\n",
    "        f\"python rescale.py {rel_prefix} --locs --radius\"\n",
    "    ]\n",
    "    \n",
    "    for cmd in cmds:\n",
    "        print(f\"Running: {cmd}\")\n",
    "        ret = subprocess.run(cmd, shell=True, cwd=str(ISTAR_ROOT), capture_output=True, text=True)\n",
    "        if ret.returncode != 0:\n",
    "            print(f\"Error running {cmd}\")\n",
    "            print(\"STDOUT:\", ret.stdout)\n",
    "            print(\"STDERR:\", ret.stderr)\n",
    "            # raise RuntimeError(f\"Failed: {cmd}\")\n",
    "\n",
    "# 预处理所有数据\n",
    "for s_id in ids:\n",
    "    if not (DATA_ROOT / s_id / 'embeddings-hist.pickle').exists():\n",
    "        run_istar_preprocess(s_id)\n",
    "    else:\n",
    "        print(f\"{s_id} already preprocessed.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INT25 already preprocessed.\n",
      "INT26 already preprocessed.\n",
      "INT27 already preprocessed.\n",
      "INT28 already preprocessed.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 模型训练 (序列化)\n",
    "\n",
    "iStar 默认是对单张切片进行 Imputation (Training & Inference)。\n",
    "为了利用多张切片的信息，我们采用序列训练策略：\n",
    "1. 在 INT25 上训练，保存 Checkpoint。\n",
    "2. 加载 INT25 Checkpoint，在 INT26 上继续训练 (Fine-tune)。\n",
    "3. 最终模型用于 INT28 预测。\n",
    "\n",
    "我们调用 `impute.py`，但需要稍作修改以支持 `load_saved` (iStar 原版支持 --load-saved 参数)。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T06:22:40.586658700Z",
     "start_time": "2026-01-19T06:22:40.557609200Z"
    }
   },
   "source": [
    "def train_istar_seq(train_ids, epochs=400):\n",
    "    # 1. Train on first slide\n",
    "    first_id = train_ids[0]\n",
    "    print(f\"Training on {first_id}...\")\n",
    "    rel_prefix_1 = os.path.relpath(DATA_ROOT / first_id, ISTAR_ROOT).replace('\\\\', '/') + '/'\n",
    "\n",
    "    # Clean previous states if any\n",
    "    # shutil.rmtree(DATA_ROOT / first_id / 'states', ignore_errors=True)\n",
    "\n",
    "    # Use sys.executable to ensure compatible environment\n",
    "    # ADDED: encoding='utf-8', errors='replace' to fix UnicodeDecodeError on Windows\n",
    "    cmd1 = f\"\\\"{sys.executable}\\\" impute.py {rel_prefix_1} --epochs={epochs} --device=cuda\"\n",
    "    print(f\"Running: {cmd1}\")\n",
    "    ret1 = subprocess.run(cmd1, shell=True, cwd=str(ISTAR_ROOT), capture_output=True, text=True, encoding='utf-8', errors='replace')\n",
    "    if ret1.returncode != 0:\n",
    "        print(f\"Error running training on {first_id}\")\n",
    "        print(\"STDOUT:\", ret1.stdout)\n",
    "        print(\"STDERR:\", ret1.stderr)\n",
    "        raise RuntimeError(\"Training failed\")\n",
    "\n",
    "    # 2. Train on subsequent slides\n",
    "    prev_id = first_id\n",
    "    for next_id in train_ids[1:]:\n",
    "        print(f\"Fine-tuning on {next_id} initialized from {prev_id}...\")\n",
    "        rel_prefix_next = os.path.relpath(DATA_ROOT / next_id, ISTAR_ROOT).replace('\\\\', '/') + '/'\n",
    "\n",
    "        # Copy states from prev to next\n",
    "        src_states = DATA_ROOT / prev_id / 'states'\n",
    "        dst_states = DATA_ROOT / next_id / 'states'\n",
    "        if dst_states.exists():\n",
    "            shutil.rmtree(dst_states)\n",
    "        shutil.copytree(src_states, dst_states)\n",
    "\n",
    "        # Resume/Finetune\n",
    "        # iStar impute.py --load-saved flag loads from {prefix}states/\n",
    "        cmd2 = f\"\\\"{sys.executable}\\\" impute.py {rel_prefix_next} --epochs={epochs} --device=cuda --load-saved\"\n",
    "        print(f\"Running: {cmd2}\")\n",
    "        ret2 = subprocess.run(cmd2, shell=True, cwd=str(ISTAR_ROOT), capture_output=True, text=True, encoding='utf-8', errors='replace')\n",
    "        if ret2.returncode != 0:\n",
    "            print(f\"Error running fine-tuning on {next_id}\")\n",
    "            print(\"STDOUT:\", ret2.stdout)\n",
    "            print(\"STDERR:\", ret2.stderr)\n",
    "            raise RuntimeError(\"Fine-tuning failed\")\n",
    "\n",
    "        prev_id = next_id\n",
    "\n",
    "    return prev_id # Last trained slide ID\n",
    "# 训练\n",
    "if not (DATA_ROOT / 'INT26' / 'states').exists():\n",
    "    last_trained_id = train_istar_seq(['INT25', 'INT26'], epochs=200)\n",
    "else:\n",
    "    print(\"Training seems done. Using INT26 states.\")\n",
    "    last_trained_id = 'INT26'"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training seems done. Using INT26 states.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 预测 (INT28)\n",
    "\n",
    "我们将训练好的模型（存在 `INT26/states` 中）复制到 `INT28/states`，然后以 `load-saved` 模式运行 `impute.py` 但 **0 epochs** (或者极少 epochs，只做预测)。\n",
    "实际上，`impute.py` 会在训练后进行预测并保存结果。\n",
    "为了只做预测，我们可以修改 `impute.py` 或者使用 trick：set epochs=0。\n",
    "如果 `epochs=0` 不工作，我们设置 `epochs=1` 但学习率极低？\n",
    "查看 `impute.py`：`train_load_model` 会执行 `trainer.fit`。如果 epochs=0，Lightning 应该会跳过训练。\n",
    "然后它调用 `predict` 函数生成 `cnts-super`。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T06:50:27.083502400Z",
     "start_time": "2026-01-19T06:22:40.587659300Z"
    }
   },
   "source": [
    "def predict_istar(model_source_id, target_id):\n",
    "    print(f\"Predicting on {target_id} using model from {model_source_id}...\")\n",
    "    rel_prefix_tgt = os.path.relpath(DATA_ROOT / target_id, ISTAR_ROOT).replace('\\\\', '/') + '/'\n",
    "    \n",
    "    # Copy states\n",
    "    src_states = DATA_ROOT / model_source_id / 'states'\n",
    "    dst_states = DATA_ROOT / target_id / 'states'\n",
    "    if dst_states.exists():\n",
    "        shutil.rmtree(dst_states)\n",
    "    shutil.copytree(src_states, dst_states)\n",
    "    \n",
    "    # Run with epochs=0 to skip training and just predict\n",
    "    cmd = f\"\\\"{sys.executable}\\\" impute.py {rel_prefix_tgt} --epochs=0 --device=cuda --load-saved\"\n",
    "    print(f\"Running: {cmd}\")\n",
    "    ret = subprocess.run(cmd, shell=True, cwd=str(ISTAR_ROOT), capture_output=True, text=True)\n",
    "    if ret.returncode != 0:\n",
    "        print(f\"Error running prediction on {target_id}\")\n",
    "        print(\"STDOUT:\", ret.stdout)\n",
    "        print(\"STDERR:\", ret.stderr)\n",
    "        raise RuntimeError(\"Prediction failed\")\n",
    "\n",
    "predict_istar(last_trained_id, 'INT28')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on INT28 using model from INT26...\n",
      "Running: \"C:\\ProgramData\\anaconda3\\envs\\istar_bench\\python.exe\" impute.py ../results/istar/data/INT28/ --epochs=0 --device=cuda --load-saved\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 评估\n",
    "\n",
    "1. 读取 `INT28` 的真实值 (`cnts.tsv`)。\n",
    "2. 读取 `INT28` 的预测值 (`cnts-super/*.pickle`)。\n",
    "3. 计算 Metrics。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T06:57:38.709234Z",
     "start_time": "2026-01-19T06:56:37.475170400Z"
    }
   },
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_istar_predictions(slide_id):\n",
    "    pred_dir = DATA_ROOT / slide_id / 'cnts-super'\n",
    "\n",
    "    # 1. Load Real Counts & Coordinates\n",
    "    # We need locs.tsv to map spots to the prediction grid\n",
    "    print(f\"Loading metadata for {slide_id}...\")\n",
    "    true_df = pd.read_csv(DATA_ROOT / slide_id / 'cnts.tsv', sep='\\t')\n",
    "    locs_df = pd.read_csv(DATA_ROOT / slide_id / 'locs.tsv', sep='\\t')\n",
    "\n",
    "    # Ensure alignment\n",
    "    true_df = true_df.set_index('SpotID')\n",
    "    locs_df = locs_df.set_index('SpotID')\n",
    "    locs_df = locs_df.loc[true_df.index]\n",
    "\n",
    "    # Get coordinates (x=col, y=row)\n",
    "    xs = locs_df['x'].values\n",
    "    ys = locs_df['y'].values\n",
    "\n",
    "    # Calculate grid indices (Factor = 16)\n",
    "    scale_factor = 16\n",
    "    grid_rows = (ys / scale_factor).astype(int)\n",
    "    grid_cols = (xs / scale_factor).astype(int)\n",
    "\n",
    "    genes = true_df.columns\n",
    "    pred_matrix = np.zeros_like(true_df.values, dtype=np.float32)\n",
    "\n",
    "    print(f\"Loading predictions for {len(genes)} genes...\")\n",
    "    missing = 0\n",
    "    max_h, max_w = 0, 0\n",
    "\n",
    "    for i, gene in enumerate(tqdm(genes)):\n",
    "        p_path = pred_dir / f\"{gene}.pickle\"\n",
    "        if p_path.exists():\n",
    "            with open(p_path, 'rb') as f:\n",
    "                val = pickle.load(f) # Shape: (864, 912)\n",
    "\n",
    "            if max_h == 0:\n",
    "                max_h, max_w = val.shape\n",
    "                # Clip indices just in case coordinates are slightly out of bound\n",
    "                grid_rows = np.clip(grid_rows, 0, max_h - 1)\n",
    "                grid_cols = np.clip(grid_cols, 0, max_w - 1)\n",
    "\n",
    "            # Sample from grid: val[row, col]\n",
    "            pred_matrix[:, i] = val[grid_rows, grid_cols]\n",
    "        else:\n",
    "            missing += 1\n",
    "\n",
    "    print(f\"Loaded {len(genes)} genes. Missing: {missing}\")\n",
    "    return pred_matrix, true_df.values, genes\n",
    "\n",
    "pred_bag, true_bag, genes = load_istar_predictions('INT28')\n",
    "\n",
    "# Metrics\n",
    "mae = np.mean(np.abs(pred_bag - true_bag))\n",
    "rmse = np.sqrt(np.mean((pred_bag - true_bag) ** 2))\n",
    "\n",
    "def pearson_corr(a, b):\n",
    "    if np.all(a == a[0]) or np.all(b == b[0]):\n",
    "        return np.nan\n",
    "    a = a - a.mean()\n",
    "    b = b - b.mean()\n",
    "    denom = np.sqrt((a * a).sum()) * np.sqrt((b * b).sum())\n",
    "    if denom == 0:\n",
    "        return np.nan\n",
    "    return float((a * b).sum() / denom)\n",
    "\n",
    "gene_corrs = []\n",
    "for i in range(pred_bag.shape[1]):\n",
    "    corr = pearson_corr(pred_bag[:, i], true_bag[:, i])\n",
    "    gene_corrs.append(corr)\n",
    "\n",
    "valid = [(i, c) for i, c in enumerate(gene_corrs) if not np.isnan(c)]\n",
    "mean_gene_corr = float(np.mean([c for _, c in valid])) if valid else float('nan')\n",
    "\n",
    "best_idx, best_corr = max(valid, key=lambda x: x[1]) if valid else (None, None)\n",
    "best_gene = genes[best_idx] if best_idx is not None else \"NA\"\n",
    "\n",
    "print(f'MAE: {mae:.4f}')\n",
    "print(f'RMSE: {rmse:.4f}')\n",
    "print(f'Mean Pearson: {mean_gene_corr:.4f}')\n",
    "print(f'Best Gene: {best_gene} ({best_corr:.4f})')\n",
    "\n",
    "metrics = {\n",
    "    'MAE': float(mae),\n",
    "    'RMSE': float(rmse),\n",
    "    'Mean_Pearson': float(mean_gene_corr) if not pd.isna(mean_gene_corr) else None,\n",
    "    'Best_Gene': best_gene,\n",
    "    'Best_Pearson': float(best_corr) if best_corr is not None else None\n",
    "}\n",
    "\n",
    "with open(RESULT_DIR / 'metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata for INT28...\n",
      "Loading predictions for 17512 genes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17512/17512 [00:49<00:00, 354.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 17512 genes. Missing: 0\n",
      "MAE: 0.9464\n",
      "RMSE: 5.9053\n",
      "Mean Pearson: 0.1846\n",
      "Best Gene: KLK3 (0.5695)\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "histogene_bench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
