{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mclSTExp Benchmark (INT25-INT28)\n",
    "\n",
    "## Environment\n",
    "mclSTExp requires PyTorch >= 2.1.0 (though 2.0 might work) and Scanpy >= 1.8.\n",
    "\n",
    "```bash\n",
    "conda create -n mcl_bench python=3.9\n",
    "conda activate mcl_bench\n",
    "pip install torch==2.1.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "pip install scanpy pandas numpy opencv-python pillow scikit-image scikit-learn seaborn matplotlib tqdm scprep timm einops\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T09:53:45.119703Z",
     "start_time": "2026-01-19T09:53:45.094683700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: D:\\code\\Morpho-VC\n",
      "MCL_ROOT: D:\\code\\Morpho-VC\\benchmark\\mclSTExp\n",
      "DATA_ROOT: D:\\code\\Morpho-VC\\benchmark\\results\\mclSTExp\\data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Set Paths\n",
    "ROOT = Path(os.environ.get('MORPHO_VC_ROOT', '../')).expanduser().resolve()\n",
    "MCL_ROOT = ROOT / 'benchmark' / 'mclSTExp'\n",
    "RESULT_DIR = ROOT / 'benchmark' / 'results' / 'mclSTExp'\n",
    "DATA_ROOT = RESULT_DIR / 'data'\n",
    "\n",
    "# sys.path.append(str(MCL_ROOT)) # We will define models inline to be self-contained\n",
    "\n",
    "RESULT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"ROOT: {ROOT}\")\n",
    "print(f\"MCL_ROOT: {MCL_ROOT}\")\n",
    "print(f\"DATA_ROOT: {DATA_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T09:53:46.756245300Z",
     "start_time": "2026-01-19T09:53:45.120728500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 17512 common genes.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import json\n",
    "import shutil\n",
    "import scprep\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load common genes\n",
    "spatial_dir = ROOT / 'data' / 'spatial_data'\n",
    "hest_dir = ROOT / 'data' / 'hest_data'\n",
    "common_gene_path = spatial_dir / 'common_genes.txt'\n",
    "with open(common_gene_path, 'r') as f:\n",
    "    common_genes = [line.strip() for line in f.readlines()]\n",
    "print(f\"Loaded {len(common_genes)} common genes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Conversion (HEST -> mclSTExp)\n",
    "\n",
    "Convert INT25-INT28 to 10x Visium format required by mclSTExp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T09:53:46.869328700Z",
     "start_time": "2026-01-19T09:53:46.838349300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting INT25...\n",
      "  Done INT25\n",
      "Converting INT26...\n",
      "  Done INT26\n",
      "Converting INT27...\n",
      "  Done INT27\n",
      "Converting INT28...\n",
      "  Done INT28\n"
     ]
    }
   ],
   "source": [
    "def convert_hest_to_mcl(slide_id):\n",
    "    print(f\"Converting {slide_id}...\")\n",
    "    sample_dir = DATA_ROOT / slide_id\n",
    "    sample_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # if (sample_dir / 'preprocessed_matrix.npy').exists():\n",
    "    #     print(f\"  {slide_id} already exists.\")\n",
    "    #     return\n",
    "    \n",
    "    adata = sc.read_h5ad(spatial_dir / f\"{slide_id}.h5ad\")\n",
    "    adata = adata[:, common_genes].copy()\n",
    "    \n",
    "    # Preprocessing (Log Norm)\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    \n",
    "    # Save Matrix (n_features, n_spots) for mclSTExp .T loading\n",
    "    processed_matrix = adata.X.toarray().T if hasattr(adata.X, 'toarray') else adata.X.T\n",
    "    np.save(sample_dir / 'preprocessed_matrix.npy', processed_matrix)\n",
    "    \n",
    "    # Barcodes\n",
    "    barcodes = pd.DataFrame(adata.obs_names)\n",
    "    barcodes.to_csv(sample_dir / 'barcodes.tsv', sep='\\t', header=False, index=False)\n",
    "    \n",
    "    # Image\n",
    "    tiff_path = hest_dir / 'wsis' / f\"{slide_id}.tif\"\n",
    "    if not tiff_path.exists(): tiff_path = hest_dir / 'wsis' / f\"{slide_id}.svs\"\n",
    "    \n",
    "    import openslide\n",
    "    slide = openslide.OpenSlide(str(tiff_path))\n",
    "    w, h = slide.dimensions\n",
    "    MAX_SIZE = 4000\n",
    "    scale = min(MAX_SIZE / w, MAX_SIZE / h)\n",
    "    level = slide.get_best_level_for_downsample(1/scale)\n",
    "    img = slide.read_region((0, 0), level, slide.level_dimensions[level]).convert('RGB')\n",
    "    true_scale_w = img.size[0] / w\n",
    "    true_scale_h = img.size[1] / h\n",
    "    img.save(sample_dir / 'image.tif')\n",
    "    \n",
    "    # Spatial Positions\n",
    "    coords = adata.obsm['spatial']\n",
    "    px_coords = coords.copy().astype(float)\n",
    "    px_coords[:, 0] *= true_scale_w # x (col)\n",
    "    px_coords[:, 1] *= true_scale_h # y (row)\n",
    "    \n",
    "    pos_df = pd.DataFrame(index=adata.obs_names)\n",
    "    pos_df['barcode'] = adata.obs_names\n",
    "    pos_df['in_tissue'] = 1\n",
    "    pos_df['array_row'] = 0 \n",
    "    pos_df['array_col'] = 0\n",
    "    pos_df['px_row'] = px_coords[:, 1].astype(int) # y\n",
    "    pos_df['px_col'] = px_coords[:, 0].astype(int) # x\n",
    "    pos_df.to_csv(sample_dir / 'tissue_positions_list.csv', header=False, index=False)\n",
    "    print(f\"  Done {slide_id}\")\n",
    "\n",
    "ids = ['INT25', 'INT26', 'INT27', 'INT28']\n",
    "for s in ids:\n",
    "    convert_hest_to_mcl(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model & Dataset Definitions\n",
    "Implemented locally to avoid dependency issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T09:53:47.143245300Z",
     "start_time": "2026-01-19T09:53:46.870053200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from torchvision.models import DenseNet121_Weights\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# --- MODEL DEFINITIONS ---\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads \n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), qkv)\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "        attn = self.attend(dots)\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class attn_block(nn.Module):\n",
    "    def __init__(self, dim, heads, dim_head, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.attn = PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout))\n",
    "        self.ff = PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
    "    def forward(self, x):\n",
    "        x = self.attn(x) + x\n",
    "        x = self.ff(x) + x\n",
    "        return x\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = models.densenet121(weights=DenseNet121_Weights.DEFAULT)\n",
    "        self.model = nn.Sequential(*list(self.model.children())[:-1])\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = True\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim, projection_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "class mclSTExp_Attention(nn.Module):\n",
    "    def __init__(self, encoder_name, temperature, image_dim, spot_dim, projection_dim, heads_num, heads_dim, head_layers, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.x_embed = nn.Embedding(65536, spot_dim)\n",
    "        self.y_embed = nn.Embedding(65536, spot_dim)\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.spot_encoder = nn.Sequential(\n",
    "            *[attn_block(spot_dim, heads=heads_num, dim_head=heads_dim, mlp_dim=spot_dim, dropout=0.) for _ in range(head_layers)]\n",
    "        )\n",
    "        self.image_projection = ProjectionHead(embedding_dim=image_dim, projection_dim=projection_dim)\n",
    "        self.spot_projection = ProjectionHead(embedding_dim=spot_dim, projection_dim=projection_dim)\n",
    "        self.temperature = temperature\n",
    "    def forward(self, batch):\n",
    "        image_features = self.image_encoder(batch[\"image\"])\n",
    "        spot_feature = batch[\"expression\"]\n",
    "        image_embeddings = self.image_projection(image_features)\n",
    "        x = batch[\"position\"][:, 0].long()\n",
    "        y = batch[\"position\"][:, 1].long()\n",
    "        centers_x = self.x_embed(x)\n",
    "        centers_y = self.y_embed(y)\n",
    "        spot_features = spot_feature + centers_x + centers_y\n",
    "        spot_features = spot_features.unsqueeze(dim=0)\n",
    "        spot_embeddings = self.spot_encoder(spot_features)\n",
    "        spot_embeddings = self.spot_projection(spot_embeddings)\n",
    "        spot_embeddings = spot_embeddings.squeeze(dim=0)\n",
    "        cos_smi = (spot_embeddings @ image_embeddings.T) / self.temperature\n",
    "        label = torch.eye(cos_smi.shape[0], cos_smi.shape[1]).to(cos_smi.device)\n",
    "        spots_loss = F.cross_entropy(cos_smi, label)\n",
    "        images_loss = F.cross_entropy(cos_smi.T, label.T)\n",
    "        loss = (images_loss + spots_loss) / 2.0\n",
    "        return loss.mean()\n",
    "\n",
    "# --- DATASET ---\n",
    "\n",
    "class CustomTenxDataset(Dataset):\n",
    "    def __init__(self, slide_id, data_root, train=True, gene_indices=None):\n",
    "        self.slide_id = slide_id\n",
    "        self.data_dir = data_root / slide_id\n",
    "        self.whole_image = cv2.imread(str(self.data_dir / 'image.tif'))\n",
    "        self.whole_image = cv2.cvtColor(self.whole_image, cv2.COLOR_BGR2RGB)\n",
    "        self.pos_df = pd.read_csv(self.data_dir / 'tissue_positions_list.csv', header=None)\n",
    "        self.barcodes = pd.read_csv(self.data_dir / 'barcodes.tsv', sep='\\t', header=None)\n",
    "        \n",
    "        # Load full matrix (Spots, Genes)\n",
    "        full_expression = np.load(self.data_dir / 'preprocessed_matrix.npy').T\n",
    "        \n",
    "        if gene_indices is not None:\n",
    "            self.expression = full_expression[:, gene_indices]\n",
    "        else:\n",
    "            self.expression = full_expression\n",
    "            \n",
    "        self.train = train\n",
    "    \n",
    "    def transform(self, image):\n",
    "        image = Image.fromarray(image)\n",
    "        if self.train:\n",
    "            if random.random() > 0.5:\n",
    "                image = TF.hflip(image)\n",
    "            if random.random() > 0.5:\n",
    "                image = TF.vflip(image)\n",
    "            angle = random.choice([180, 90, 0, -90])\n",
    "            image = TF.rotate(image, angle)\n",
    "        return np.asarray(image)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.barcodes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        row = self.pos_df.iloc[idx]\n",
    "        v1 = int(row[4]) \n",
    "        v2 = int(row[5]) \n",
    "        r = 112\n",
    "        img_h, img_w, _ = self.whole_image.shape\n",
    "        y_min = max(0, v1 - r)\n",
    "        y_max = min(img_h, v1 + r)\n",
    "        x_min = max(0, v2 - r)\n",
    "        x_max = min(img_w, v2 + r)\n",
    "        patch = self.whole_image[y_min:y_max, x_min:x_max]\n",
    "        if patch.shape[0] < 224 or patch.shape[1] < 224:\n",
    "            patch = np.pad(patch, ((0, 224 - patch.shape[0]), (0, 224 - patch.shape[1]), (0, 0)), mode='constant')\n",
    "        patch = self.transform(patch)\n",
    "        item['image'] = torch.tensor(patch).permute(2, 0, 1).float() / 255.0\n",
    "        item['expression'] = torch.tensor(self.expression[idx, :]).float()\n",
    "        item['barcode'] = self.barcodes.values[idx, 0]\n",
    "        item['position'] = torch.tensor([v1, v2]).float()\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training & Prediction Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T09:53:47.177837900Z",
     "start_time": "2026-01-19T09:53:47.144781400Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_mcl_chunk(train_slides, val_slides, gene_indices, epochs=50, batch_size=32, device='cuda', patience=5, min_delta=0.001):\n",
    "    # Train Data\n",
    "    datasets = [CustomTenxDataset(s, DATA_ROOT, train=True, gene_indices=gene_indices) for s in train_slides]\n",
    "    train_ds = torch.utils.data.ConcatDataset(datasets)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Validation Data (INT27) - train=False 用于关闭数据增强\n",
    "    val_datasets = [CustomTenxDataset(s, DATA_ROOT, train=False, gene_indices=gene_indices) for s in val_slides]\n",
    "    val_ds = torch.utils.data.ConcatDataset(val_datasets)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    n_genes = len(gene_indices)\n",
    "    \n",
    "    model = mclSTExp_Attention(\n",
    "        encoder_name='densenet121', temperature=0.1, \n",
    "        image_dim=1024, spot_dim=n_genes, projection_dim=256,\n",
    "        heads_num=8, heads_dim=64, head_layers=2\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    # 早停变量\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # print(f\"  Start training for {epochs} epochs (Patience: {patience})...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # --- Training ---\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"    Epoch {epoch+1}/{epochs} [Train]\", leave=False, ncols=100)\n",
    "        \n",
    "        for batch in pbar:\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if k in ['image', 'expression', 'position']}\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'t_loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items() if k in ['image', 'expression', 'position']}\n",
    "                loss = model(batch)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # 打印 (只在 Epoch 1, 10, 20... 或早停时打印)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"    Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}\")\n",
    "\n",
    "        # === Val Loss Early Stopping ===\n",
    "        if avg_val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"    [Early Stopping] Stopped at epoch {epoch+1}. Best Val Loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "            \n",
    "    return model\n",
    "\n",
    "def predict_mcl_chunk(model, train_slides, target_slide, gene_indices, device='cuda', top_k=50):\n",
    "    model.eval()\n",
    "    \n",
    "    # Target Embeddings\n",
    "    target_ds = CustomTenxDataset(target_slide, DATA_ROOT, train=False, gene_indices=gene_indices)\n",
    "    target_loader = DataLoader(target_ds, batch_size=64, shuffle=False)\n",
    "    \n",
    "    target_img_embs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in target_loader:\n",
    "            imgs = batch['image'].to(device)\n",
    "            feats = model.image_encoder(imgs)\n",
    "            embs = model.image_projection(feats)\n",
    "            target_img_embs.append(embs.cpu())\n",
    "    target_img_embs = torch.cat(target_img_embs, dim=0)\n",
    "\n",
    "    # Key Embeddings\n",
    "    train_datasets = [CustomTenxDataset(s, DATA_ROOT, train=False, gene_indices=gene_indices) for s in train_slides]\n",
    "    train_ds = torch.utils.data.ConcatDataset(train_datasets)\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=False)\n",
    "    \n",
    "    key_spot_embs = []\n",
    "    key_expressions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in train_loader:\n",
    "            exp = batch['expression'].to(device)\n",
    "            pos = batch['position'].to(device)\n",
    "            c_x = model.x_embed(pos[:, 0].long())\n",
    "            c_y = model.y_embed(pos[:, 1].long())\n",
    "            spot_feat = exp + c_x + c_y\n",
    "            enc_feat = model.spot_encoder(spot_feat.unsqueeze(0))\n",
    "            proj_feat = model.spot_projection(enc_feat).squeeze(0)\n",
    "            key_spot_embs.append(proj_feat.cpu())\n",
    "            key_expressions.append(exp.cpu())\n",
    "            \n",
    "    key_spot_embs = torch.cat(key_spot_embs, dim=0)\n",
    "    key_expressions = torch.cat(key_expressions, dim=0)\n",
    "    \n",
    "    # Matching\n",
    "    Q = F.normalize(target_img_embs.to(device), p=2, dim=1)\n",
    "    K = F.normalize(key_spot_embs.to(device), p=2, dim=1)\n",
    "    V = key_expressions.to(device)\n",
    "    \n",
    "    final_preds = []\n",
    "    chunk_size = 500\n",
    "    for i in range(0, len(Q), chunk_size):\n",
    "        q_chunk = Q[i:i+chunk_size]\n",
    "        sim = q_chunk @ K.T\n",
    "        sim = torch.clamp(sim, -1.0, 0.9999)\n",
    "        dist_sq = 2 * (1 - sim)\n",
    "        weights = (1.0 / dist_sq)\n",
    "        \n",
    "        top_w, top_idx = torch.topk(weights, k=top_k, dim=1)\n",
    "        vals = V[top_idx]\n",
    "        norm_w = top_w / top_w.sum(dim=1, keepdim=True)\n",
    "        chunk_pred = (vals * norm_w.unsqueeze(2)).sum(dim=1)\n",
    "        final_preds.append(chunk_pred.cpu())\n",
    "        \n",
    "    return torch.cat(final_preds, dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T10:04:22.848233400Z",
     "start_time": "2026-01-19T09:53:47.193397600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Genes: 17512\n",
      "Loading Ground Truth Data...\n",
      "Ground Truth Loaded.\n",
      "\n",
      "=== Processing Chunk 0-3000 (3000 genes) ===\n",
      "  [Cache] Found existing result: D:\\code\\Morpho-VC\\benchmark\\results\\mclSTExp\\preds_chunk_0_3000.npy\n",
      "  Loading from cache...\n",
      "\n",
      "=== Processing Chunk 3000-6000 (3000 genes) ===\n",
      "  [Cache] Found existing result: D:\\code\\Morpho-VC\\benchmark\\results\\mclSTExp\\preds_chunk_3000_6000.npy\n",
      "  Loading from cache...\n",
      "\n",
      "=== Processing Chunk 6000-9000 (3000 genes) ===\n",
      "  [Cache] Found existing result: D:\\code\\Morpho-VC\\benchmark\\results\\mclSTExp\\preds_chunk_6000_9000.npy\n",
      "  Loading from cache...\n",
      "\n",
      "=== Processing Chunk 9000-12000 (3000 genes) ===\n",
      "  [Cache] Found existing result: D:\\code\\Morpho-VC\\benchmark\\results\\mclSTExp\\preds_chunk_9000_12000.npy\n",
      "  Loading from cache...\n",
      "\n",
      "=== Processing Chunk 12000-15000 (3000 genes) ===\n",
      "  [Cache] Found existing result: D:\\code\\Morpho-VC\\benchmark\\results\\mclSTExp\\preds_chunk_12000_15000.npy\n",
      "  Loading from cache...\n",
      "\n",
      "=== Processing Chunk 15000-17512 (2512 genes) ===\n",
      "  [Cache] Found existing result: D:\\code\\Morpho-VC\\benchmark\\results\\mclSTExp\\preds_chunk_15000_17512.npy\n",
      "  Loading from cache...\n",
      "\n",
      "All chunks processed. Concatenating results...\n",
      "Pred Shape: (3990, 17512), GT Shape: (3990, 17512)\n",
      "Calculating Pearson Correlation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calc PCC: 100%|██████████| 17512/17512 [00:02<00:00, 7655.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.2065\n",
      "RMSE: 0.3756\n",
      "Mean Pearson: 0.0555\n",
      "Metrics saved to D:\\code\\Morpho-VC\\benchmark\\results\\mclSTExp\\metrics.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import json\n",
    "\n",
    "train_slides = ['INT25', 'INT26']\n",
    "val_slides = ['INT27']   # 新增验证集\n",
    "target_slide = 'INT28'\n",
    "\n",
    "# Need to know total genes first. Read one dataset headers\n",
    "temp_ds = CustomTenxDataset('INT25', DATA_ROOT)\n",
    "# 修正：直接读取 shape，避免加载整个矩阵\n",
    "total_genes = temp_ds.expression.shape[1]\n",
    "print(f\"Total Genes: {total_genes}\")\n",
    "del temp_ds\n",
    "\n",
    "chunk_size = 3000\n",
    "all_preds = []\n",
    "all_indices = []\n",
    "\n",
    "# Ground Truth load ONCE\n",
    "print(\"Loading Ground Truth Data...\")\n",
    "target_ds_full = CustomTenxDataset(target_slide, DATA_ROOT, train=False)\n",
    "ground_truth = target_ds_full.expression # Full genes\n",
    "barcodes_gt = target_ds_full.barcodes\n",
    "print(\"Ground Truth Loaded.\")\n",
    "\n",
    "# 修正循环逻辑\n",
    "for start_idx in range(0, total_genes, chunk_size):\n",
    "    end_idx = min(start_idx + chunk_size, total_genes)\n",
    "    indices = np.arange(start_idx, end_idx)\n",
    "    result_path = RESULT_DIR / f\"preds_chunk_{start_idx}_{end_idx}.npy\"\n",
    "\n",
    "    print(f\"\\n=== Processing Chunk {start_idx}-{end_idx} ({len(indices)} genes) ===\")\n",
    "\n",
    "    if result_path.exists():\n",
    "        print(f\"  [Cache] Found existing result: {result_path}\")\n",
    "        print(\"  Loading from cache...\")\n",
    "        preds = np.load(result_path)\n",
    "    else:\n",
    "        print(\"  [Train] No cache found. Starting training chunk model...\")\n",
    "        # Train (Pass val_slides now)\n",
    "        model = train_mcl_chunk(train_slides, val_slides, indices, epochs=50, batch_size=32)\n",
    "\n",
    "        print(\"  [Predict] Predicting target slide...\")\n",
    "        # Predict\n",
    "        preds = predict_mcl_chunk(model, train_slides, target_slide, indices)\n",
    "        np.save(result_path, preds)\n",
    "        print(f\"  [Save] Saved chunk result to {result_path}\")\n",
    "\n",
    "        # Cleanup\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"  [Clean] Memory cleared.\")\n",
    "\n",
    "    all_preds.append(preds)\n",
    "    all_indices.append(indices)\n",
    "\n",
    "print(\"\\nAll chunks processed. Concatenating results...\")\n",
    "# Concatenate\n",
    "full_preds = np.concatenate(all_preds, axis=1) # (N_spots, N_genes)\n",
    "\n",
    "# Metrics\n",
    "print(f\"Pred Shape: {full_preds.shape}, GT Shape: {ground_truth.shape}\")\n",
    "mae = np.mean(np.abs(full_preds - ground_truth))\n",
    "rmse = np.sqrt(np.mean((full_preds - ground_truth) ** 2))\n",
    "\n",
    "def pearson_corr(a, b):\n",
    "    # 避免常数数组导致的除零错误\n",
    "    if np.all(a == a[0]) or np.all(b == b[0]): return np.nan\n",
    "    a, b = a - a.mean(), b - b.mean()\n",
    "    denom = np.sqrt((a*a).sum()) * np.sqrt((b*b).sum())\n",
    "    return (a*b).sum() / denom if denom != 0 else np.nan\n",
    "\n",
    "print(\"Calculating Pearson Correlation...\")\n",
    "pccs = [pearson_corr(full_preds[:, i], ground_truth[:, i]) for i in tqdm(range(full_preds.shape[1]), desc=\"Calc PCC\")]\n",
    "mean_pcc = np.nanmean(pccs)\n",
    "\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"Mean Pearson: {mean_pcc:.4f}\")\n",
    "\n",
    "metrics = {'MAE': float(mae), 'RMSE': float(rmse), 'Mean_Pearson': float(mean_pcc)}\n",
    "with open(RESULT_DIR / 'metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "print(f\"Metrics saved to {RESULT_DIR / 'metrics.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.2065\n",
      "RMSE: 0.3756\n",
      "Mean Pearson: 0.0555\n"
     ]
    }
   ],
   "source": [
    "# 修改这里：使用上一个Cell已经计算好的 full_preds 和 ground_truth\n",
    "preds = full_preds\n",
    "truths = ground_truth\n",
    "\n",
    "# 下面的代码保持不变\n",
    "# Metrics\n",
    "mae = np.mean(np.abs(preds - truths))\n",
    "rmse = np.sqrt(np.mean((preds - truths) ** 2))\n",
    "\n",
    "def pearson_corr(a, b):\n",
    "    if np.all(a == a[0]) or np.all(b == b[0]): return np.nan\n",
    "    a, b = a - a.mean(), b - b.mean()\n",
    "    denom = np.sqrt((a*a).sum()) * np.sqrt((b*b).sum())\n",
    "    return (a*b).sum() / denom if denom != 0 else np.nan\n",
    "\n",
    "pccs = [pearson_corr(preds[:, i], truths[:, i]) for i in range(preds.shape[1])]\n",
    "mean_pcc = np.nanmean(pccs)\n",
    "\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"Mean Pearson: {mean_pcc:.4f}\")\n",
    "\n",
    "metrics = {'MAE': float(mae), 'RMSE': float(rmse), 'Mean_Pearson': float(mean_pcc)}\n",
    "with open(RESULT_DIR / 'metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "istar_bench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
